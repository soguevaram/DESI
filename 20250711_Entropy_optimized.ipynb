{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22a96bf8-0e7c-436a-b611-b0b638af7122",
   "metadata": {},
   "source": [
    "<h2 style=\"font-size:24pt\"> Proyecto DESI</h2>\n",
    "\n",
    "<h2 style=\"font-size:24pt\"> Julio 11, 2025</h2>\n",
    "\n",
    "<p style=\"font-size:16pt\">\n",
    "Calculation of the optimized entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f8b11fe-9315-4a7c-b201-0f13338ed635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.tri as mtri\n",
    "from scipy.spatial import Delaunay\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import Table\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "from scipy.stats import norm\n",
    "from mpl_toolkits.mplot3d import Axes3D \n",
    "from sklearn.decomposition import PCA\n",
    "from functools import reduce\n",
    "import glob\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re\n",
    "from astropy.io import ascii\n",
    "from itertools import combinations\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88ec2fdc-cf51-454f-9bc9-6532a9163201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8.19 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rosettes = list(range(20))\n",
    "data = {}\n",
    "\n",
    "for number_rosette in range(20):\n",
    "    file = f\"data_rosette/LRG_{number_rosette}_clustering_data.ecsv\"\n",
    "    table = Table.read(file, format=\"ascii.ecsv\") \n",
    "    subset = table[['TARGETID','RA', 'DEC', 'Z','x','y','z']].to_pandas()\n",
    "    data[f\"data_{number_rosette}\"] = subset\n",
    "    data[f\"data_{number_rosette}\"]['type'] = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d67f19-63a4-4c50-8eb7-70e37c8bd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_r(data_all):\n",
    "    df_tri = data_all[['x', 'y', 'z']].values\n",
    "    tri = Delaunay(df_tri)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    ids = data_all['TARGETID'].values\n",
    "    types = data_all['type'].values\n",
    "\n",
    "    # Crear nodos con atributos\n",
    "    for coords, tipo, id_ in zip(df_tri, types, ids):\n",
    "        G.add_node(id_, pos=tuple(coords), type=tipo)\n",
    "\n",
    "    # Conectar nodos usando TARGETID\n",
    "    G.add_edges_from(\n",
    "        (ids[simplex[i]], ids[simplex[j]])\n",
    "        for simplex in tri.simplices\n",
    "        for i in range(3)\n",
    "        for j in range(i + 1, 4)\n",
    "    )\n",
    "\n",
    "    # Calcular degree\n",
    "    degree_dict = dict(G.degree())\n",
    "    data_all['degree'] = data_all['TARGETID'].map(degree_dict)\n",
    "\n",
    "    # Contar vecinos por tipo\n",
    "    n_data_dict = {}\n",
    "    n_random_dict = {}\n",
    "\n",
    "    for node in G.nodes:\n",
    "        neighbors = list(G.neighbors(node))\n",
    "        neighbor_types = [G.nodes[n]['type'] for n in neighbors]\n",
    "        n_data_dict[node] = neighbor_types.count('data')\n",
    "        n_random_dict[node] = neighbor_types.count('rand')\n",
    "\n",
    "    data_all['N_data'] = data_all['TARGETID'].map(n_data_dict)\n",
    "    data_all['N_random'] = data_all['TARGETID'].map(n_random_dict)\n",
    "\n",
    "    # Calcular r\n",
    "    total = data_all['N_data'] + data_all['N_random']\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        r = np.where(total > 0, (data_all['N_data'] - data_all['N_random']) / total, 0)\n",
    "\n",
    "    data_all['r'] = r\n",
    "\n",
    "    return data_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4dd4ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_r(df):\n",
    "    coords = df[['x', 'y', 'z']].values\n",
    "    types = df['type'].values\n",
    "\n",
    "    tri = Delaunay(coords)\n",
    "\n",
    "    # Adjacency list for neighbors\n",
    "    neighbors = {i: set() for i in range(len(coords))}\n",
    "    for simplex in tri.simplices:\n",
    "        for i, j in combinations(simplex, 2):\n",
    "            neighbors[i].add(j)\n",
    "            neighbors[j].add(i)\n",
    "\n",
    "    r = np.zeros(len(coords), dtype=float)\n",
    "\n",
    "    for i, nbrs in neighbors.items():\n",
    "        nbrs = list(nbrs)\n",
    "        type_neighbors = types[nbrs]  # extraer los tipos de los vecinos\n",
    "        n_data = np.sum(type_neighbors == 'data')\n",
    "        n_rand = len(nbrs) - n_data\n",
    "\n",
    "        if (n_data + n_rand) > 0:\n",
    "            r[i] = (n_data - n_rand) / (n_data + n_rand)\n",
    "        else:\n",
    "            raise ValueError(f'No neighbors for point {i} in the triangulation.')\n",
    "\n",
    "    out = df.copy()\n",
    "    out['r'] = r\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4f89932-b071-48e3-8090-af0c441fb80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification(data,number_rand):\n",
    "\n",
    "    data.loc[(data['r'] >= -1.0) & (data['r'] <= -0.9), f'class_{number_rand}'] = 'void'\n",
    "    data.loc[(data['r'] >  -0.9) & (data['r'] <=  0.0), f'class_{number_rand}'] = 'sheet'\n",
    "    data.loc[(data['r'] >   0.0) & (data['r'] <=  0.9), f'class_{number_rand}'] = 'filament'\n",
    "    data.loc[(data['r'] >   0.9) & (data['r'] <=  1.0), f'class_{number_rand}'] = 'knot'\n",
    "\n",
    "    data.sort_values('z', inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1de00e1-cd57-4dc2-948d-7b0c1ff771c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(data):\n",
    "    count = data[data.columns].apply(lambda row: row.value_counts(), axis=1)\n",
    "    probabilidades = count.div(number_random)\n",
    "    sum_entropy = probabilidades.apply(lambda row: np.sum(row[row > 0] * np.log2(row[row > 0])), axis=1)\n",
    "    entropy = -(1/np.log2(4))*sum_entropy\n",
    "    entropy = np.abs(entropy)\n",
    "    return entropy,probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6f6dcc0d-2233-46ef-a0b3-6aa7900dc187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rosette 0\n",
      "Rosette 1\n",
      "Rosette 2\n",
      "Rosette 3\n",
      "Rosette 4\n",
      "Rosette 5\n",
      "Rosette 6\n",
      "Rosette 7\n",
      "Rosette 8\n",
      "Rosette 9\n",
      "Rosette 10\n",
      "Rosette 11\n",
      "Rosette 12\n",
      "Rosette 13\n",
      "Rosette 14\n",
      "Rosette 15\n",
      "Rosette 16\n",
      "Rosette 17\n",
      "Rosette 18\n",
      "Rosette 19\n",
      "CPU times: total: 41min 3s\n",
      "Wall time: 43min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rand = {}\n",
    "rand_list = {}\n",
    "df_merge = {}\n",
    "df_entropy = {}\n",
    "number_random = 100\n",
    "\n",
    "for i in range(20):\n",
    "    rand_list[i] = []\n",
    "    print(f'Rosette {i}')\n",
    "    \n",
    "    for j in range(number_random):\n",
    "\n",
    "        #Read files\n",
    "        file = f'rand_rosette/LRG_rosette_{i}_random_{j}.ecsv'\n",
    "        table = Table.read(file, format=\"ascii.ecsv\") \n",
    "        subset = table[['TARGETID', 'RA', 'DEC', 'Z', 'x', 'y', 'z']].to_pandas()\n",
    "        subset['type'] = 'rand'\n",
    "\n",
    "        #Concat real and random data\n",
    "        df_concat = pd.concat([subset, data[f'data_{i}'].copy().assign(type='data')], ignore_index=True)\n",
    "\n",
    "        #Parameter r and classification\n",
    "        #data_with_r = parameter_r(df_concat)\n",
    "        data_with_r = compute_r(df_concat)\n",
    "        data_with_class = classification(data_with_r, j)\n",
    "        \n",
    "        rand_list[i].append(data_with_class)\n",
    "\n",
    "    #Merge of the same rossete\n",
    "    df_merge[i] = data[f'data_{i}'][['TARGETID']].copy()\n",
    "    for j in range(number_random):\n",
    "        df_j = rand_list[i][j][['TARGETID', f'class_{j}']]\n",
    "        df_merge[i] = df_merge[i].merge(df_j, on='TARGETID', how='left')\n",
    "\n",
    "    #Entropy\n",
    "    reset = entropy(df_merge[i].set_index('TARGETID'))\n",
    "    reset_entropy = reset[0].rename('entropy').reset_index()\n",
    "    df_merge[i] = df_merge[i].merge(reset_entropy, on='TARGETID', how='left')\n",
    "    df_entropy[i] = df_merge[i][['TARGETID','entropy']].copy()\n",
    "\n",
    "    #Save file of entropy\n",
    "    table_entropy = Table.from_pandas(df_entropy[i])\n",
    "    filename = f\"entropy_rosette/LRG_entropy_rosette_{i}.ecsv\"\n",
    "    ascii.write(table_entropy, filename, format='ecsv', overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
